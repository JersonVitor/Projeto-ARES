{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "703QLhtPFopT"
      },
      "source": [
        "## Imports e installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6FBiR_dK6Dt6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting InquirerPy\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy)\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in c:\\users\\jerso\\appdata\\roaming\\python\\python312\\site-packages (from InquirerPy) (3.0.50)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\jerso\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy) (0.2.13)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: pfzy, InquirerPy\n",
            "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#%pip install torch\n",
        "%pip install InquirerPy\n",
        "#%pip install kagglehub\n",
        "#%pip install pandas\n",
        "#%pip install torchvision\n",
        "#%pip install ipywidgets\n",
        "#%pip install scikit-learn seaborn\n",
        "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r68yJGOAFM4r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import time\n",
        "import torch\n",
        "import logging\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib as ptl\n",
        "import tqdm as notebook_tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMbMovrdFgTb"
      },
      "source": [
        "## Baixando Base de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzMCzXiuFf4D",
        "outputId": "3e0b43c8-22b9-4277-b173-c64915c9fc29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/davimedio01/v-librasil?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10.1G/10.1G [02:17<00:00, 78.8MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/davimedio01/v-librasil/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"davimedio01/v-librasil\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcxMyUGx6Dt9"
      },
      "source": [
        "## Variáveis Globais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rmjrN0-T6Dt9"
      },
      "outputs": [],
      "source": [
        "videos_path = \"../../videos/\"\n",
        "csv_path = \"../../videos/annotations.csv\"\n",
        "csv_path_features = \"../../features/annotations.csv\"\n",
        "videos_path_teste = \"../../features_teste/\"\n",
        "csv_path_teste = \"../../features_teste/annotations.csv\"\n",
        "videos_path_val = \"../../features_val/\"\n",
        "csv_path_val = \"../../features_val/annotations.csv\"\n",
        "features_path = \"../../features/\"   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfwAkUIB6Dt-"
      },
      "source": [
        "## Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L3tjntLI6Dt-"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(\"FeatureExtraction\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "fh = logging.FileHandler(\"feature_extraction.log\")\n",
        "fh.setLevel(logging.DEBUG)\n",
        "\n",
        "fmt = logging.Formatter(\n",
        "    \"%(asctime)s — %(name)s — %(levelname)s — %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "ch.setFormatter(fmt)\n",
        "fh.setFormatter(fmt)\n",
        "logger.addHandler(ch)\n",
        "logger.addHandler(fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treinamento CNN e RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR80zn6PF1gr"
      },
      "source": [
        "## Dataloaders para CNN e RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRYjmbzqF0pa"
      },
      "outputs": [],
      "source": [
        "class CNNDataset(Dataset):\n",
        "  def __init__(self, annotations_file, videosDir, transform=None, target_transform=None):\n",
        "    self.annotations_file = annotations_file\n",
        "    self.labels = self.getLabels()\n",
        "    classes_em_ordem = list(dict.fromkeys(self.labels[\"class\"]))\n",
        "    self.label2idx = {classe: i for i, classe in enumerate(classes_em_ordem)}\n",
        "    self.idx2label = {i: classe for classe, i in self.label2idx.items()}\n",
        "    self.videos_name = self.getVideosName()\n",
        "    self.videosDir = videosDir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    video = self.extractFrames(self.videosDir + self.videos_name.iloc[idx,0])\n",
        "    label = self.label2idx[self.labels.iloc[idx,0]]\n",
        "    video_name = self.videos_name.iloc[idx,0]\n",
        "    frames_t = [self.transform(frame) for frame in video]  # cada frame→Tensor[C,H,W]\n",
        "    videoT = torch.stack(frames_t, dim=0)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    return videoT, label, video_name\n",
        "\n",
        "  def getLabels(self):\n",
        "    labels = pd.read_csv(self.annotations_file)\n",
        "    return labels[[\"class\"]]\n",
        "\n",
        "  def getVideosName(self):\n",
        "    videosName = pd.read_csv(self.annotations_file)\n",
        "    return videosName[[\"video_name\"]]\n",
        "\n",
        "  def idxToLabel(self,idx):\n",
        "    return self.idx2label[idx]\n",
        "\n",
        "  def extractFrames(self,filepath):\n",
        "    video = cv2.VideoCapture(filepath)\n",
        "    frames = []\n",
        "    while video.isOpened():\n",
        "      sucess, frame = video.read()\n",
        "      if not sucess:\n",
        "        break\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      frames.append(frame)\n",
        "    video.release()\n",
        "    return frames\n",
        "\n",
        "def collate_fn(batch):\n",
        "  sequences,labels, video_name = zip(*batch)\n",
        "  lengths = [seq.shape[0] for seq in sequences]\n",
        "  padded_sequences = pad_sequence(sequences=sequences,batch_first=True, padding_value=0)\n",
        "  labels = torch.tensor(labels, dtype=torch.long)\n",
        "  return padded_sequences,labels,video_name,lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SRDlV1EP5CM6"
      },
      "outputs": [],
      "source": [
        "class RNNDataset(Dataset):\n",
        "  def __init__(self, annotations_file,featuresDir, transform=None, target_transform=None):\n",
        "    self.annotations_file = annotations_file\n",
        "    self.labels = self.getLabels()\n",
        "    classes_em_ordem = list(dict.fromkeys(self.labels[\"class\"]))\n",
        "    self.label2idx = {classe: i for i, classe in enumerate(classes_em_ordem)}\n",
        "    self.idx2label = {i: classe for classe, i in self.label2idx.items()}\n",
        "    self.features = self.getFeaturesNames()\n",
        "    self.featuresDir = featuresDir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    feature_data = self.extractFeature(self.featuresDir + self.features.iloc[idx, 0])\n",
        "    features = feature_data['features']\n",
        "    length = feature_data['length']  # comprimento real (sem padding)\n",
        "    features = features[:length]  # remove o padding\n",
        "    label = self.label2idx[self.labels.iloc[idx,0]]\n",
        "    if self.transform:\n",
        "      features = self.transform(features)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    return features, label\n",
        "\n",
        "  def getFeaturesNames(self):\n",
        "    df = pd.read_csv(self.annotations_file)\n",
        "    df[\"video_name\"] = df[\"video_name\"].str.replace(\"mp4\", \"pt\", regex=False)\n",
        "    return df[[\"video_name\"]]\n",
        "\n",
        "  def getLabels(self):\n",
        "    df = pd.read_csv(self.annotations_file)\n",
        "    return df[[\"class\"]]\n",
        "  \n",
        "  \n",
        "  def extractFeature(self, path):\n",
        "    return torch.load(path)\n",
        "\n",
        "def rnn_collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    lengths = [seq.shape[0] for seq in sequences]\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return padded_sequences, labels, lengths\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVRW46D_6Dt_"
      },
      "source": [
        "## Criação de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxjR87KI6DuA"
      },
      "source": [
        "### Modelo CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CNNMobileNetV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Carrega MobileNetV2 pré‑treinado\n",
        "        backbone = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "        # Congela todos os parâmetros\n",
        "        for p in backbone.parameters():\n",
        "            p.requires_grad = False\n",
        "          # 3) Extrai apenas as camadas convolucionais\n",
        "        #    backbone.features é um nn.Sequential que vai até antes do classifier\n",
        "        self.features = backbone.features\n",
        "\n",
        "        # 4) Define um pool adaptativo para reduzir [B,1280,H,W] → [B,1280,1,1]\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)           \n",
        "        x = self.pool(x)              \n",
        "        x = x.view(x.size(0), -1)       \n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6zqm1ul6DuA"
      },
      "source": [
        "### Modelo RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout = 0.3):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Camada GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        #self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Empacotar sequências para otimização\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            x,\n",
        "            lengths=lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        \n",
        "        packed_output, hidden = self.gru(packed)\n",
        "        last_hidden = hidden[-1] \n",
        "\n",
        "        # Aplica dropout sobre esse vetor\n",
        "        #last_hidden = self.dropout(last_hidden)\n",
        "\n",
        "        # Classificador final\n",
        "        out = self.fc(last_hidden)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkEBMIh56DuB"
      },
      "source": [
        "## Extração de características das imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BecB6RtQ6DuB"
      },
      "source": [
        "### Configuração de paralelismo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpdk1ZiR6DuB",
        "outputId": "3e25f3b0-3018-473a-8457-ab0d66903f3b"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"O Resnet irá trabalhar em GPU?{torch.cuda.is_available()}\")\n",
        "\n",
        "\n",
        "# Define quantas threads usar para intra-op parallelism (ex.: convoluções)\n",
        "torch.set_num_threads(4)\n",
        "torch.set_num_interop_threads(2)\n",
        "\n",
        "num_workersCNN = 0\n",
        "batch_sizeCNN = 8\n",
        "pin_memory = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh6vDzFg6DuB"
      },
      "source": [
        "### Configuração de Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z1U9EeGe6DuC"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((112,112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "datasetCNN = CNNDataset(\n",
        "    annotations_file=csv_path,\n",
        "    videosDir=videos_path,\n",
        "    transform= transform\n",
        ")\n",
        "loaderCNN = DataLoader(\n",
        "    datasetCNN,\n",
        "    batch_size=batch_sizeCNN,\n",
        "    num_workers=num_workersCNN,          # menos processos\n",
        "    pin_memory=pin_memory,       # desabilita se não for usar GPU\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E05yEpGu6DuC"
      },
      "source": [
        "### Executar CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-RbjZ166DuC",
        "outputId": "008d6792-1690-4c84-9967-1d1fd4b1ae89"
      },
      "outputs": [],
      "source": [
        "numberDivTeste = 5\n",
        "numberDivVal = 3\n",
        "\n",
        "\n",
        "extrator = CNNMobileNetV2().to(device=device)\n",
        "extrator.eval()\n",
        "logger.info(\"Iniciando extração de features usando MobileNetV2\")\n",
        "logger.info('-' * 50)\n",
        "with torch.no_grad():\n",
        "    for batch_idx,(videos, labels, names,lengths) in enumerate(tqdm(loaderCNN, desc=\"Extraindo batches\")):\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            B,T,C,H,W = videos.shape\n",
        "            logger.info(f\"Batch {batch_idx}: shape vídeos = {videos.shape}\")\n",
        "            flat = videos.view(B*T,C,H,W).to(device)\n",
        "            tqdm.write(\"frames extraídos\")\n",
        "            feats_flats = extrator(flat)\n",
        "            D = feats_flats.size(1)\n",
        "            feats = feats_flats.view(B,T,D).cpu()\n",
        "\n",
        "            for i in range(B):\n",
        "                class_name = datasetCNN.idxToLabel(labels[i].item())\n",
        "                video_name = names[i].replace(\".mp4\", \"\")\n",
        "                real_T = lengths[i] \n",
        "                x = re.search('-',video_name)\n",
        "                chave = int(video_name[x.end()])\n",
        "                if chave % numberDivVal == 0:\n",
        "                    out_path = f\"{videos_path_val}{video_name}.pt\"\n",
        "                    torch.save({\n",
        "                        'features': feats[i],\n",
        "                        'length': real_T\n",
        "                    }, out_path)\n",
        "                elif chave % numberDivTeste == 0:\n",
        "                    out_path = f\"{videos_path_teste}{video_name}.pt\"\n",
        "                    torch.save({\n",
        "                        'features': feats[i],\n",
        "                        'length': real_T\n",
        "                    }, out_path)\n",
        "                else:\n",
        "                    out_path = f\"{features_path}{video_name}.pt\"\n",
        "                    torch.save({\n",
        "                        'features': feats[i],\n",
        "                        'length': real_T\n",
        "                    }, out_path)\n",
        "            # métricas: tempo e throughput\n",
        "            elapsed = time.time() - start_time\n",
        "            videos_per_sec = B / elapsed\n",
        "            frames_per_sec = (B * T) / elapsed\n",
        "            logger.info(f\"Batch {batch_idx} processado em {elapsed:.2f}s — \")\n",
        "            logger.info(f\"{videos_per_sec:.1f} vídeos/s, {frames_per_sec:.1f} frames/s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro no batch {batch_idx}: {e}\", exc_info=True)\n",
        "            \n",
        "\n",
        "features_name = os.listdir(features_path)\n",
        "features_name_teste = os.listdir(videos_path_teste)\n",
        "features_name_val = os.listdir(videos_path_val)\n",
        "feature_base = []\n",
        "class_base = []\n",
        "feature_teste = []\n",
        "class_teste = []\n",
        "numberDivTeste = 5\n",
        "class_val = []\n",
        "feature_val = []\n",
        "numberDivVal = 3\n",
        "for cl in features_name:\n",
        "    feature_base.append(cl)\n",
        "    pos = re.search('Sinalizador',cl) \n",
        "    class_base.append(cl[2:pos.start()])\n",
        "df = pd.DataFrame({\n",
        "    \"video_name\": feature_base,\n",
        "    \"class\": class_base,\n",
        "  })\n",
        "df.to_csv(csv_path_features,index=False)   \n",
        " \n",
        "for cl in features_name_teste:\n",
        "    feature_teste.append(cl)\n",
        "    pos = re.search('Sinalizador',cl) \n",
        "    class_teste.append(cl[2:pos.start()])\n",
        "df = pd.DataFrame({\n",
        "    \"video_name\": feature_teste,\n",
        "    \"class\": class_teste,\n",
        "  })\n",
        "df.to_csv(csv_path_teste,index=False)   \n",
        " \n",
        "for cl in features_name_val:\n",
        "    feature_val.append(cl)\n",
        "    pos = re.search('Sinalizador',cl) \n",
        "    class_val.append(cl[2:pos.start()])\n",
        "    \n",
        "df = pd.DataFrame({\n",
        "    \"video_name\": feature_val,\n",
        "    \"class\": class_val,\n",
        "  })\n",
        "df.to_csv(csv_path_val,index=False)    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Treino do RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuração de paralelismo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_workersRNN = 0\n",
        "batch_sizeRNN = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuração de Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn_dataset = RNNDataset(\n",
        "    annotations_file=csv_path_features,\n",
        "    featuresDir=features_path\n",
        ")\n",
        "\n",
        "rnn_loader = DataLoader(\n",
        "    rnn_dataset,\n",
        "    batch_size = batch_sizeRNN,\n",
        "    shuffle = True,\n",
        "    pin_memory = pin_memory,\n",
        "    collate_fn = rnn_collate_fn,\n",
        "    num_workers = num_workersRNN\n",
        ")\n",
        "rnn_dataset_val = RNNDataset(\n",
        "    annotations_file=csv_path_val,\n",
        "    featuresDir=videos_path_val\n",
        ")\n",
        "\n",
        "rnn_loader_val = DataLoader(\n",
        "    rnn_dataset_val,\n",
        "    batch_size = batch_sizeRNN,\n",
        "    shuffle = True,\n",
        "    pin_memory= pin_memory,\n",
        "    collate_fn = rnn_collate_fn,\n",
        "    num_workers = num_workersRNN\n",
        ")\n",
        "rnn_dataset_teste = RNNDataset(\n",
        "    annotations_file=csv_path_teste,\n",
        "    featuresDir=videos_path_teste\n",
        ")\n",
        "\n",
        "rnn_loader_teste = DataLoader(\n",
        "    rnn_dataset_val,\n",
        "    batch_size = batch_sizeRNN,\n",
        "    shuffle = True,\n",
        "    pin_memory = pin_memory,\n",
        "    collate_fn = rnn_collate_fn,\n",
        "    num_workers = num_workersRNN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM = 1280  # Dimensão dos features da MobileNetV2\n",
        "HIDDEN_DIM = 512\n",
        "NUM_LAYERS = 2\n",
        "NUM_CLASSES = 20\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# Criação do modelo\n",
        "gru_model = GRUModel(\n",
        "    input_dim=INPUT_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    dropout= DROPOUT\n",
        ").to(device)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=15):\n",
        "    logger.info(\"Treinamento RNN\")\n",
        "    logger.info('-' * 50)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "    # Listas para armazenar métricas\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Treino\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for sequences, labels, lengths in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(sequences, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Estatísticas\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Métricas de treino\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_acc = 100 * correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "        \n",
        "        # Validação\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for sequences, labels, lengths in val_loader:\n",
        "                sequences = sequences.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                outputs = model(sequences, lengths)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                val_running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Métricas de validação\n",
        "        epoch_val_loss = val_running_loss / len(val_loader)\n",
        "        #scheduler.step(epoch_val_loss)\n",
        "        epoch_val_acc = 100 * val_correct / val_total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "        \n",
        "        logger.info(f'Epoch {epoch+1}')\n",
        "        logger.info(f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}%')\n",
        "        logger.info(f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%')\n",
        "        logger.info('-' * 50)\n",
        "    \n",
        "    # Plotar curvas\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Curva de Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.title('Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Curva de Acurácia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Val Accuracy')\n",
        "    plt.title('Accuracy Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return model, train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "# Iniciar treinamento\n",
        "model_trained, train_loss, train_acc, val_loss, val_acc = train_model(gru_model,rnn_loader,rnn_loader_val,num_epochs=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'train_loss': train_loss,\n",
        "    'train_acc': train_acc,\n",
        "    'val_loss': val_loss,\n",
        "    'val_acc': val_acc\n",
        "}\n",
        "torch.save(metrics, 'training_metrics.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, dataset, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for sequences, labels, lengths in tqdm(dataloader, desc=\"Gerando matriz de confusão\"):\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(sequences, lengths)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Obter classes e nomes\n",
        "    classes = sorted(dataset.label2idx.keys())\n",
        "    class_names = [dataset.idx2label[i] for i in range(len(classes))]\n",
        "    \n",
        "    # Calcular matriz de confusão\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    \n",
        "    # Normalizar por linha (por classe real)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    # Plotar\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    \n",
        "    plt.title('Matriz de Confusão Normalizada')\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Salvar versão não normalizada também\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    \n",
        "    plt.title('Matriz de Confusão (Contagens Absolutas)')\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Mostrar relatório de classificação\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=class_names,\n",
        "        zero_division=0\n",
        "    ))\n",
        "    \n",
        "    return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supondo que você já tenha:\n",
        "# - model_trained: seu modelo treinado\n",
        "# - test_loader: DataLoader de teste\n",
        "# - dataset: instância original do dataset (para mapeamento de classes)\n",
        "\n",
        "# Gerar e plotar a matriz\n",
        "conf_matrix = plot_confusion_matrix(\n",
        "    model=model_trained,\n",
        "    dataloader=rnn_loader_real_teste,\n",
        "    dataset=rnn_dataset_teste,  # Passar o dataset original para mapear os labels\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import threading\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "# Configurações\n",
        "SEQUENCE_LENGTH = 50\n",
        "IMG_SIZE = 112\n",
        "CONFIDENCE_THRESHOLD = 0.4\n",
        "\n",
        "class GesturePredictor:\n",
        "    def __init__(self, cnn_path, gru_path, label_map):\n",
        "        # Carregar modelos otimizados\n",
        "        self.cnn_model = torch.jit.load(cnn_path, map_location=device).half().eval()\n",
        "        self.gru_model = torch.jit.load(gru_path, map_location=device).half().eval()\n",
        "        \n",
        "        self.label_map = label_map\n",
        "        self.frame_queue = deque(maxlen=SEQUENCE_LENGTH)\n",
        "        self.prediction_queue = deque(maxlen=5)\n",
        "        self.lock = threading.Lock()\n",
        "        \n",
        "        # Pré-processamento\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "            transforms.Grayscale(num_output_channels=3),  # Manter 3 canais\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        # Thread de processamento\n",
        "        self.processing_thread = None\n",
        "        self.current_frame = None\n",
        "        self.running = True\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        # Converter para RGB e redimensionar\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "        \n",
        "      \n",
        "        # Converter para tensor\n",
        "        tensor = self.transform(frame).half().unsqueeze(0)\n",
        "        return tensor.to(device)\n",
        "\n",
        "    def process_frames(self):\n",
        "        while self.running:\n",
        "            if self.current_frame is not None:\n",
        "                with self.lock:\n",
        "                    frame = self.current_frame.copy()\n",
        "                    self.current_frame = None\n",
        "                \n",
        "                processed_tensor = self.preprocess_frame(frame)\n",
        "                with torch.no_grad():\n",
        "                    features = self.cnn_model(processed_tensor)\n",
        "                \n",
        "                self.frame_queue.append(features)\n",
        "\n",
        "    def predict_gesture(self):\n",
        "        if len(self.frame_queue) < SEQUENCE_LENGTH:\n",
        "            return \"Coletando frames...\", 0.0, (0, 0, 0)\n",
        "        \n",
        "        features = list(self.frame_queue)\n",
        "        features_tensor = torch.cat(features).unsqueeze(0)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            gru_out = self.gru_model(features_tensor, torch.tensor([SEQUENCE_LENGTH]))\n",
        "            probabilities = F.softmax(gru_out, dim=1)\n",
        "            confidence, pred_idx = torch.max(probabilities, 1)\n",
        "        \n",
        "        confidence = confidence.item()\n",
        "        pred_idx = pred_idx.item()\n",
        "        self.prediction_queue.append(pred_idx)\n",
        "        \n",
        "        # Filtro temporal\n",
        "        final_pred = max(set(self.prediction_queue), key=self.prediction_queue.count)\n",
        "        \n",
        "        # Mapear cor\n",
        "        color = (0, 255, 0) if confidence > CONFIDENCE_THRESHOLD else (0, 0, 255)\n",
        "        \n",
        "        return self.label_map[final_pred], confidence, color\n",
        "\n",
        "    def run(self):\n",
        "        self.processing_thread = threading.Thread(target=self.process_frames)\n",
        "        self.processing_thread.start()\n",
        "        \n",
        "        cap = cv2.VideoCapture(0)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "        \n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            with self.lock:\n",
        "                self.current_frame = frame.copy()\n",
        "            \n",
        "            # Obter predição\n",
        "            label, confidence, color = self.predict_gesture()\n",
        "            \n",
        "            # Interface\n",
        "            cv2.putText(frame, f\"{label} ({confidence:.2f})\", \n",
        "                        (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
        "            \n",
        "            cv2.imshow('Reconhecimento de Gestos - Frame Completo', frame)\n",
        "            \n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        \n",
        "        self.running = False\n",
        "        self.processing_thread.join()\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "#%% Funções para salvar modelos e mapeamento\n",
        "def save_models(cnn_model, gru_model, label_map, save_dir=\"saved_models\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Salvar modelos com TorchScript\n",
        "    cnn_scripted = torch.jit.script(cnn_model.cpu().eval())\n",
        "    gru_scripted = torch.jit.script(gru_model.cpu().eval())\n",
        "    \n",
        "    # Salvar arquivos\n",
        "    torch.save({\n",
        "        'cnn_state_dict': cnn_model.state_dict(),\n",
        "        'gru_state_dict': gru_model.state_dict(),\n",
        "        'label_map': label_map\n",
        "    }, os.path.join(save_dir, \"models_checkpoint.pth\"))\n",
        "    \n",
        "    cnn_scripted.save(os.path.join(save_dir, \"cnn_model.pt\"))\n",
        "    gru_scripted.save(os.path.join(save_dir, \"gru_model.pt\"))\n",
        "    \n",
        "    print(f\"Modelos salvos em: {save_dir}\")\n",
        "\n",
        "def load_models(save_dir=\"saved_models\", device=device):\n",
        "    # Carregar mapeamento de labels\n",
        "    checkpoint = torch.load(os.path.join(save_dir, \"models_checkpoint.pth\"), map_location=device)\n",
        "    label_map = checkpoint['label_map']\n",
        "    \n",
        "    # Carregar arquivos TorchScript\n",
        "    cnn_model = torch.jit.load(os.path.join(save_dir, \"cnn_model.pt\"), map_location=device)\n",
        "    gru_model = torch.jit.load(os.path.join(save_dir, \"gru_model.pt\"), map_location=device)\n",
        "    \n",
        "    # Carregar estados para treino continuado (opcional)\n",
        "    cnn_model.load_state_dict(checkpoint['cnn_state_dict'])\n",
        "    gru_model.load_state_dict(checkpoint['gru_state_dict'])\n",
        "    \n",
        "    return cnn_model, gru_model, label_map\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelos salvos em: saved_models\n"
          ]
        }
      ],
      "source": [
        "extrator = CNNMobileNetV2().to(device=device)\n",
        "save_models(extrator,model_trained,datasetCNN.idx2label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'datasetCNN' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m extrator = CNNMobileNetV2().to(device=device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m save_models(extrator,model_trained,\u001b[43mdatasetCNN\u001b[49m.idx2label)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Carregar dataset para pegar o mapeamento correto\u001b[39;00m\n\u001b[32m      4\u001b[39m dataset = CNNDataset(\n\u001b[32m      5\u001b[39m     annotations_file=csv_path,\n\u001b[32m      6\u001b[39m     videosDir=videos_path,\n\u001b[32m      7\u001b[39m     transform=transforms.Compose([transforms.ToPILImage(), transforms.Resize((\u001b[32m112\u001b[39m,\u001b[32m112\u001b[39m)), transforms.ToTensor()])\n\u001b[32m      8\u001b[39m )\n",
            "\u001b[31mNameError\u001b[39m: name 'datasetCNN' is not defined"
          ]
        }
      ],
      "source": [
        "extrator = CNNMobileNetV2().to(device=device)\n",
        "save_models(extrator,model_trained,datasetCNN.idx2label)\n",
        "# 1. Carregar dataset para pegar o mapeamento correto\n",
        "dataset = CNNDataset(\n",
        "    annotations_file=csv_path,\n",
        "    videosDir=videos_path,\n",
        "    transform=transforms.Compose([transforms.ToPILImage(), transforms.Resize((112,112)), transforms.ToTensor()])\n",
        ")\n",
        "\n",
        "# 2. Criar/Carregar modelos\n",
        "if os.path.exists(\"saved_models\"):\n",
        "    print(\"Carregando modelos pré-treinados...\")\n",
        "    cnn_model, gru_model, label_map = load_models()\n",
        "else:\n",
        "    print(\"Inicializando novos modelos...\")\n",
        "    # Exemplo de criação dos modelos\n",
        "    cnn_model = CNNMobileNetV2().to(device)\n",
        "    gru_model = GRUModel(\n",
        "        input_dim=1280,\n",
        "        hidden_dim=512,\n",
        "        num_layers=2,\n",
        "        num_classes=len(dataset.label2idx)\n",
        "    ).to(device)\n",
        "    \n",
        "    # Salvar com mapeamento original do dataset\n",
        "    save_models(cnn_model, gru_model, dataset.idx2label)\n",
        "\n",
        "# 3. Verificar mapeamento\n",
        "\n",
        "\n",
        "# 4. Inicializar predictor\n",
        "predictor = GesturePredictor(\n",
        "    cnn_path=os.path.join(\"saved_models\", \"cnn_model.pt\"),\n",
        "    gru_path=os.path.join(\"saved_models\", \"gru_model.pt\"),\n",
        "    label_map=label_map\n",
        ")\n",
        "\n",
        "# 5. Executar\n",
        "print(\"\\nIniciando reconhecimento... (Pressione 'q' para sair)\")\n",
        "predictor.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rMbMovrdFgTb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
